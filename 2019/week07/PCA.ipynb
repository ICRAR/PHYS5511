{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 7 - PCA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ICRAR/PHYS5511/blob/master/2019/week07/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH4YULBKhVC3",
        "colab_type": "text"
      },
      "source": [
        "# Original Code\n",
        "https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySM0otjxMZMJ",
        "colab_type": "text"
      },
      "source": [
        "# Breast Cancer Data\n",
        "\n",
        "The Breast Cancer data set is a real-valued multivariate data that consists of two classes, where each class signifies whether a patient has breast cancer or not. The two categories are: malignant and benign.\n",
        "\n",
        "The malignant class has 212 samples, whereas the benign class has 357 samples.\n",
        "\n",
        "It has 30 features shared across all classes: radius, texture, perimeter, area, smoothness, fractal dimension, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ-m9O89Mp4-",
        "colab_type": "text"
      },
      "source": [
        "Let's first explore the Breast Cancer dataset.\n",
        "\n",
        "You will use sklearn's module datasets and import the Breast Cancer dataset from it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXU41K05Nikp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ7ggjRCNlZF",
        "colab_type": "text"
      },
      "source": [
        "`load_breast_cancer` will give you both labels and the data. To fetch the data, you will call `.data` and for fetching the labels .target`.\n",
        "\n",
        "The data has 569 samples with thirty features, and each sample has a label associated with it. There are two labels in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3TvOVz1OHfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast = load_breast_cancer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh36Yi6dOJVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_data = breast.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATBNW1KHOX2p",
        "colab_type": "text"
      },
      "source": [
        "Let's check the shape of the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUhuk7fKOgqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewaCxzP9Omj-",
        "colab_type": "text"
      },
      "source": [
        "Even though for this tutorial, you do not need the labels but still for better understanding, let's load the labels and check the shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tINQCIF3Oz8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_labels = breast.target\n",
        "breast_labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL2ar9ufOvuV",
        "colab_type": "text"
      },
      "source": [
        "Now you will `import numpy` since you will be reshaping the `breast_labels` to concatenate it with the breast_data so that you can finally create a `DataFrame` which will have both the data and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC1PcBLvO6j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "labels = np.reshape(breast_labels,(569,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhtRbHDO_eE",
        "colab_type": "text"
      },
      "source": [
        "After `reshaping` the labels, you will `concatenate` the data and labels along the second axis, which means the final shape of the array will be 569 x 31.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXIE5KT-PhuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_breast_data = np.concatenate([breast_data,labels],axis=1)\n",
        "final_breast_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtEM_BJZPvM4",
        "colab_type": "text"
      },
      "source": [
        "Now you will import `pandas` to create the `DataFrame` of the final data to represent the data in a tabular fashion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt7ffisTP5aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "breast_dataset = pd.DataFrame(final_breast_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewLGKtK2QB4m",
        "colab_type": "text"
      },
      "source": [
        "Let's quickly print the features that are there in the breast cancer dataset!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjbduaiQEIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = breast.feature_names\n",
        "features\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeHbfh9EQG6t",
        "colab_type": "text"
      },
      "source": [
        "If you note in the `features` array, the `label` field is missing. Hence, you will have to manually add it to the `features` array since you will be equating this array with the column names of your `breast_dataset` dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCl2FRSpQIg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_labels = np.append(features,'label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F84fIFvmQK2c",
        "colab_type": "text"
      },
      "source": [
        "Now you will embed the column names to the `breast_dataset` dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAltdMK2QMoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_dataset.columns = features_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeUOT_-DQN8d",
        "colab_type": "text"
      },
      "source": [
        "Let's print the first few rows of the dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQDTP9nQPcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo9Vo7N6Q8GK",
        "colab_type": "text"
      },
      "source": [
        "Since the original labels are in `0,1` format, you will change the labels to `benign` and `malignant` using `.replace` function. You will use `inplace=True` which will modify the dataframe `breast_dataset`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXzqxY-lRCYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_dataset['label'].replace(0, 'Benign',inplace=True)\n",
        "breast_dataset['label'].replace(1, 'Malignant',inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvqqfbnRQ_SV",
        "colab_type": "text"
      },
      "source": [
        "Let's print the last few rows of the `breast_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNTEUU4_Q9v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_dataset.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ph5bZcRbNO",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the Breast Cancer data\n",
        "You start by Standardising the data since PCA's output is influenced based on the scale of the features of the data.\n",
        "\n",
        "It is a common practice to normalize your data before feeding it to any machine learning algorithm.\n",
        "\n",
        "To apply normalization, you will import `StandardScaler` module from the sklearn library and select only the features from the `breast_dataset` you created in the Data Exploration step. Once you have the features, you will then apply scaling by doing `fit_transform` on the feature data.\n",
        "\n",
        "While applying `StandardScaler`, each feature of your data should be normally distributed such that it will scale the distribution to a mean of zero and a standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwJlWCCsR6jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "x = breast_dataset.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x) # normalising the features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0OFgHFgSAVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tHWb0XCSFGA",
        "colab_type": "text"
      },
      "source": [
        "Let's check whether the normalised data has a mean of zero and a standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruO36iOoSKmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(x), np.std(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WetUCQgaSULe",
        "colab_type": "text"
      },
      "source": [
        "Let's convert the normalised features into a tabular format with the help of DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1fu51zLSShX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "normalised_breast = pd.DataFrame(x,columns=feat_cols)\n",
        "normalised_breast.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWzv2DGOSnFM",
        "colab_type": "text"
      },
      "source": [
        "Now comes the critical part, the next few lines of code will be projecting the thirty-dimensional Breast Cancer data to two-dimensional **principal components**.\n",
        "\n",
        "You will use the sklearn library to import the PCA module, and in the PCA method, you will pass the number of components (n_components=2) and finally call fit_transform on the aggregate data. Here, several components represent the lower dimension in which you will project your higher dimension data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-xJAD6-SwZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_breast = PCA(n_components=2)\n",
        "principalComponents_breast = pca_breast.fit_transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6PUwn2bSyu7",
        "colab_type": "text"
      },
      "source": [
        "Next, let's create a DataFrame that will have the principal component values for all 569 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ombsyBjoS4eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "principal_breast_Df = pd.DataFrame(data = principalComponents_breast, columns = ['principal component 1', 'principal component 2'])\n",
        "principal_breast_Df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_imRK3nGS_Gx",
        "colab_type": "text"
      },
      "source": [
        "Once you have the principal components, you can find the `explained_variance_ratio`. It will provide you with the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meiHqk_TTLmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Explained variation per principal component: {pca_breast.explained_variance_ratio_}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt3U7HL8ThHG",
        "colab_type": "text"
      },
      "source": [
        "From the above output, you can observe that the *principal component 1* holds 44.2% of the information while the *principal component 2* holds only 19% of the information. Also, the other point to note is that while projecting thirty-dimensional data to a two-dimensional data, 36.8% information was lost.\n",
        "\n",
        "Let's plot the visualization of the 569 samples along the *principal component - 1* and *principal component - 2* axis. It should give you good insight into how your samples are distributed among the two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf6YL2orT3ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMhpXuh9Trns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.xlabel('Principal Component - 1',fontsize=20)\n",
        "plt.ylabel('Principal Component - 2',fontsize=20)\n",
        "plt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\n",
        "targets = ['Benign', 'Malignant']\n",
        "colors = ['r', 'g']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = breast_dataset['label'] == target\n",
        "    plt.scatter(principal_breast_Df.loc[indicesToKeep, 'principal component 1']\n",
        "               , principal_breast_Df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
        "\n",
        "plt.legend(targets,prop={'size': 15})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR7Q83ojUCYi",
        "colab_type": "text"
      },
      "source": [
        "From the above graph, you can observe that the two classes **benign** and **malignant**, when projected to a two-dimensional space, can be linearly separable up to some extent. Other observations can be that the **benign** class is spread out as compared to the **malignant** class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ZaKYBpUZW3",
        "colab_type": "text"
      },
      "source": [
        "# CIFAR - 10\n",
        "The CIFAR-10 (Canadian Institute For Advanced Research) dataset consists of 60000 images each of 32x32x3 color images having ten classes, with 6000 images per category.\n",
        "\n",
        "The dataset consists of 50000 training images and 10000 test images.\n",
        "\n",
        "The classes in the dataset are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYfjMs8EUmi9",
        "colab_type": "text"
      },
      "source": [
        "You can load the CIFAR - 10 dataset using Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M5ZkrnPUz6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjrU6vdAUylU",
        "colab_type": "text"
      },
      "source": [
        "Once imported, you will use the `.load_data()` method to download the data, it will download and store the data in your Keras directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om4No1IMUxQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ydEnqxUwPT",
        "colab_type": "text"
      },
      "source": [
        "The above line of code returns training and test images along with the labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG1xbeJRUugz",
        "colab_type": "text"
      },
      "source": [
        "Let's quickly print the shape of training and testing images shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyfbjd8eUtfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Traning data shape:', x_train.shape)\n",
        "print('Testing data shape:', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3tH4Gi3UsFO",
        "colab_type": "text"
      },
      "source": [
        "Let's also print the shape of the labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym_rLcB3UqR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train.shape,y_test.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNFxatVtUo4d",
        "colab_type": "text"
      },
      "source": [
        "Let's also find out the total number of labels and the various kinds of classes the data has."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9u3fPzjVeu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zonP9u_6Vkx2",
        "colab_type": "text"
      },
      "source": [
        "For a better understanding, let's create a dictionary that will have class names with their corresponding categorical class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJfYM36KWu4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = {\n",
        " 0: 'airplane',\n",
        " 1: 'automobile',\n",
        " 2: 'bird',\n",
        " 3: 'cat',\n",
        " 4: 'deer',\n",
        " 5: 'dog',\n",
        " 6: 'frog',\n",
        " 7: 'horse',\n",
        " 8: 'ship',\n",
        " 9: 'truck',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwNaZFdaW42d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "curr_img = np.reshape(x_train[0], (32,32,3))\n",
        "plt.imshow(curr_img)\n",
        "print(plt.title(\"(Label: \" + str(label_dict[y_train[0][0]]) + \")\"))\n",
        "\n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "curr_img = np.reshape(x_test[0],(32,32,3))\n",
        "plt.imshow(curr_img)\n",
        "print(plt.title(\"(Label: \" + str(label_dict[y_test[0][0]]) + \")\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSVb27gFXF27",
        "colab_type": "text"
      },
      "source": [
        "Even though the above two images are blurry, you can still somehow observe that the first image is a *frog* with the label *frog*, while the second image is of a *cat* with the label *cat*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRtBZoOzXRfy",
        "colab_type": "text"
      },
      "source": [
        "The following lines of code for visualizing the CIFAR-10 data is pretty similar to the PCA visualization of the Breast Cancer data.\n",
        "\n",
        "Let's quickly check the maximum and minimum values of the CIFAR-10 training images and normalize the pixels between 0 and 1 inclusive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A5AnJ2qXXm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.min(x_train), np.max(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADYafydRXfhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train/255.0\n",
        "np.min(x_train),np.max(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S-FEyzxXmHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMUPDynXrRv",
        "colab_type": "text"
      },
      "source": [
        "Next, you will create a DataFrame that will hold the pixel values of the images along with their respective labels in a row-column format.\n",
        "\n",
        "But before that, let's reshape the image dimensions from three to one (flatten the images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo6xXoMhXxR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_flat = x_train.reshape(-1, 32 * 32 * 3)\n",
        "feat_cols = ['pixel'+str(i) for i in range(x_train_flat.shape[1])]\n",
        "df_cifar = pd.DataFrame(x_train_flat,columns=feat_cols)\n",
        "df_cifar['label'] = y_train\n",
        "print('Size of the dataframe: {}'.format(df_cifar.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BvT_4dqYsOP",
        "colab_type": "text"
      },
      "source": [
        "The size of the dataframe is correct since there are 50,000 training images, each having 3072 pixels and an additional column for labels so in total 3073.\n",
        "\n",
        "PCA will be applied on all the columns except the last one, which is the label for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-wcMAO6Yx9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cifar.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rs0_qctY3g2",
        "colab_type": "text"
      },
      "source": [
        "Next, you will create the PCA method and pass the number of components as two and apply `fit_transform` on the training data, this can take few seconds since there are 50,000 samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6E7rXFY-ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_cifar = PCA(n_components=2)\n",
        "principalComponents_cifar = pca_cifar.fit_transform(df_cifar.iloc[:,:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UA1-WEDZEVc",
        "colab_type": "text"
      },
      "source": [
        "Then you will convert the principal components for each of the 50,000 images from a numpy array to a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbmIwlnqZIoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "principal_cifar_Df = pd.DataFrame(data = principalComponents_cifar, columns = ['principal component 1', 'principal component 2'])\n",
        "principal_cifar_Df['y'] = y_train\n",
        "principal_cifar_Df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGdMTXMOZkeR",
        "colab_type": "text"
      },
      "source": [
        "Let's quickly find out the amount of information or variance the principal components hold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49YH-4PsZmQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Explained variation per principal component: {pca_cifar.explained_variance_ratio_}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrc2NyHMZ1c-",
        "colab_type": "text"
      },
      "source": [
        "Well, it looks like a decent amount of information was retained by the principal components 1 and 2, given that the data was projected from 3072 dimensions to a mere two principal components.\n",
        "\n",
        "It's time to visualize the CIFAR-10 data in a two-dimensional space. Remember that there is some semantic class overlap in this dataset which means that a frog can have a slightly similar shape of a cat or a deer with a dog; especially when projected in a two-dimensional space. The differences between them might not be captured that well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYbyPiZRaCnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(\n",
        "    x=\"principal component 1\", y=\"principal component 2\",\n",
        "    hue=\"y\",\n",
        "    palette=sns.color_palette(\"hls\", 10),\n",
        "    data=principal_cifar_Df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-WQ9rD2aLmd",
        "colab_type": "text"
      },
      "source": [
        "From the above figure, you can observe that some variation was captured by the principal components since there is some structure in the points when projected along the two principal component axis. The points belonging to the same class are close to each other, and the points or images that are very different semantically are further away from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCWLm_AamrB",
        "colab_type": "text"
      },
      "source": [
        "# Speed Up Deep Learning Training using PCA with CIFAR - 10 Dataset\n",
        "\n",
        "Now let's speed up your Deep Learning Model's training process using PCA.\n",
        "\n",
        "First, let's normalize the training and testing images. If you remember the training images were normalized in the PCA visualization part, so you only need to normalize the testing images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G117ukaYa6CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test/255.0\n",
        "x_test = x_test.reshape(-1,32,32,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzdJq0iLbEfG",
        "colab_type": "text"
      },
      "source": [
        "Let's reshape the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1HdSJ0BbGJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_flat = x_test.reshape(-1,3072)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoCNzgTEbKoH",
        "colab_type": "text"
      },
      "source": [
        "Next, you will make the instance of the PCA model.\n",
        "\n",
        "Here, you can also pass how much variance you want PCA to capture. Let's pass 0.9 as a parameter to the PCA model, which means that PCA will hold 90% of the variance and the number of components required to capture 90% variance will be used.\n",
        "\n",
        "Note that earlier you passed n_components as a parameter and you could then find out how much variance was captured by those two components. But here we explicitly mention how much variance we would like PCA to capture and hence, the n_components will vary based on the variance parameter.\n",
        "\n",
        "If you do not pass any variance, then the number of components will be equal to the original dimension of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsbpTEb5bXOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFmwGoqTbbgb",
        "colab_type": "text"
      },
      "source": [
        "Then you will fit the PCA instance on the training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiu1a4C6bdPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.fit(x_train_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiKI-0P3br0k",
        "colab_type": "text"
      },
      "source": [
        "Now let's find out how many n_components PCA used to capture 0.9 variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWNbNSUvb2Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.n_components_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ejHMr0b_2a",
        "colab_type": "text"
      },
      "source": [
        "From the above output, you can observe that to achieve 90% variance, the dimension was reduced to 99 principal components from the actual 3072 dimensions.\n",
        "\n",
        "Finally, you will apply transform on both the training and test set to generate a transformed dataset from the parameters generated from the fit method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt9zL_nGcHtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_img_pca = pca.transform(x_train_flat)\n",
        "test_img_pca = pca.transform(x_test_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkidXYTxcPLq",
        "colab_type": "text"
      },
      "source": [
        "Next, let's quickly import the necessary libraries to run the deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7XUZU5cZmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U72ZzIBjcf1w",
        "colab_type": "text"
      },
      "source": [
        "Now, you will convert your training and testing labels to one-hot encoding vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWdgCSs9cjaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_SEH8Socq-v",
        "colab_type": "text"
      },
      "source": [
        "Let's define the number of epochs, number of classes, and the batch size for your model; and the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-sX20ycqGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(99,)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zdO38qdc1RF",
        "colab_type": "text"
      },
      "source": [
        "Let's print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8pbWb1Nc5UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKBR2MvvdAZk",
        "colab_type": "text"
      },
      "source": [
        "Finally, it's time to compile and train the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjC5222dEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=RMSprop(),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_img_pca, \n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=(test_img_pca, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOs6ZdUdWgJ",
        "colab_type": "text"
      },
      "source": [
        "From the above output, you can observe that the time taken for training each epoch was just 20 seconds on a CPU. The model did a decent job on the training data, achieving 90ish% accuracy while it achieved only 55ish% accuracy on the test data (don't forget each of your runs will be different to mine). This means that it overfitted the training data. However, remember that the data was projected to 99 dimensions from 3072 dimensions and despite that it did a great job!\n",
        "\n",
        "Finally, let's see how much time the model takes to train on the original dataset and how much accuracy it can achieve using the same deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd0mBm2ldX6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(3072,)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=RMSprop(),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_flat, \n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=(x_test_flat, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iONvZhrduRi",
        "colab_type": "text"
      },
      "source": [
        "From the above output, it is quite evident that the time taken for training each epoch was around 45 seconds on a CPU which was two times more than the model trained on the PCA output.\n",
        "\n",
        "Moreover, both the training and testing accuracy is less than the accuracy you achieved with the 99 principal components as an input to the model.\n",
        "\n",
        "So, by applying PCA on the training data you were able to train your deep learning algorithm not only fast, but it also achieved better accuracy on the testing data when compared with the deep learning algorithm trained with original training data."
      ]
    }
  ]
}