{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_two_baseline_add_pos.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ICRAR/PHYS5511/blob/master/2019/assignments/assignment_two_baseline_add_pos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMpLdOpKd9qH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The second baseline solution that has a better score than [the first \"naive\" solution](https://colab.research.google.com/github/ICRAR/PHYS5511/blob/master/2019/assignments/assignment_two_baseline.ipynb). The major difference is that this version uses the location information presented in the distance table.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llajr0Rhx7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7L0Svfg7UZ",
        "colab_type": "text"
      },
      "source": [
        "#Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rahiH7cIJP0z",
        "colab_type": "text"
      },
      "source": [
        "The same usual \"business\" as Assignment One"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yFaWUahduJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRuIx4lNgzu1",
        "colab_type": "text"
      },
      "source": [
        "##Get the data for the first time\n",
        "You only need to run this once, and this step can be skipped for subsequent runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpQommDeeuZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My\\ Drive/PHYS5512/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35fjqmFfCoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir isfog2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLu5HikNfHJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd isfog2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyXSkdrlf1OL",
        "colab_type": "text"
      },
      "source": [
        "Make sure you already have the *kaggle.json* file, which you probably downloaded from Kaggle for assignment one. If not, you can download it from your Kaggle profile again. Then we copy that to the /root/.kaggle directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeDOJTnEfkgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ../../kaggle.json /root/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmIK-zB9fJ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c isfog2020-pile-driving-predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37FGA4mGfbMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH-H-0-tik8v",
        "colab_type": "text"
      },
      "source": [
        "##Goto the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fT-F3CVijXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My Drive/PHYS5512/data/isfog2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCQjm1SKhx7V",
        "colab_type": "text"
      },
      "source": [
        "#Pre-process data\n",
        "\n",
        "This part of the code of checking data is copied from the original kernel.\n",
        "\n",
        "The dataset is kindly provided by [Cathie Group](http://www.cathiegroup.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upigVCmq9QnZ",
        "colab_type": "text"
      },
      "source": [
        "##Importing data\n",
        "\n",
        "The first step in any data science exercise is to get familiar with the data. The data is provided in a csv file (```training_data.csv```). We can import the data with Pandas and display the first five rows using the ```head()``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctL_coJEhx7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"training_data_cleaned.csv\")  # Store the contents of the csv file in the variable 'train_df'\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6IAjIU3hx7Y",
        "colab_type": "text"
      },
      "source": [
        "The data has 12 columns, containing PCPT data ($ q_c $, $ f_s $ and $ u_2 $), recorded hammer data (blowcount, normalised hammer energy, normalised ENTHRU and total number of blows), pile data (diameter, bottom wall thickness and pile final penetration). A unique ID identifies the location and $ z $ defines the depth below the mudline.\n",
        "\n",
        "The data has already been resampled to a regular grid with 0.5m grid intervals to facilitate the further data handling. Note that the \"grid\" axis is the depth.\n",
        "\n",
        "The hammer energy has been normalised using the same reference energy for all piles in this prediction exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NxCv5sdgUEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_full_df = pd.read_csv(\"full_cpt_training_data.csv.zip\")  # Store the contents of the csv file in the variable 'train_df'\n",
        "train_full_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LxtRrvq3sN4",
        "colab_type": "text"
      },
      "source": [
        "As we can see, a lot of CPT data records are not included in the main training table. We will need to use this information to improve our solution. Because deep learning is hungery for data, and the existing main training table has only a very small number of measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkIHtd1bgi9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_full_df = pd.read_csv(\"full_cpt_validation_data.csv.zip\")  # Store the contents of the csv file in the variable 'train_df'\n",
        "test_full_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ForaqA-4JF-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.groupby('Location ID')['Location ID'].agg(['count']).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jlH47qHob2f",
        "colab_type": "text"
      },
      "source": [
        "The above code checks how many records per location, here we only show the first 10 locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2WYB_6NrkUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_locs = train_df['Location ID'].unique()\n",
        "print(train_locs, len(train_locs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxeEZjOov4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit = pd.read_csv(\"sample_submission.csv\")\n",
        "df_submit.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD9wkwGH5S9g",
        "colab_type": "text"
      },
      "source": [
        "The submission contains two columns. The first column is a contatination the location ID of the wind turbine and the pile depth. The second column is the predicted blowcount per meter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhG-gVtJFlUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(df_submit), df_submit.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7nXISc9570V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(\"validation_data_cleaned.csv\")\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHyU5Kb8eT7",
        "colab_type": "text"
      },
      "source": [
        "The test dataset 10 columns - the PCPT data ($ q_c $, $ f_s $ and $ u_2 $), **incomplete** hammer data (normalised hammer energy, normalised ENTHRU), pile data (diameter, bottom wall thickness and pile final penetration). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SEYeKsVpAkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.groupby('Location ID')['Location ID'].agg(['count']).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzyU6qwBsKMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_locs = test_df['Location ID'].unique()\n",
        "print(test_locs, len(test_locs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHbgPdxmI_Ec",
        "colab_type": "text"
      },
      "source": [
        "The above code checks unique locations included in the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyaaz3w9pNiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_df = pd.read_csv(\"interdistance_data.csv\")\n",
        "dist_df.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ACCatU7lh50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inter_dist_class = dist_df['Interdistance class'].unique()\n",
        "print(inter_dist_class, len(inter_dist_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY4eNOU9meWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loc1 = dist_df['ID location 1'].unique()\n",
        "print(loc1, len(loc1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58AxK0Fo9F6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_df.loc[dist_df['ID location 1'] == loc1[0]].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA283z-iABQy",
        "colab_type": "text"
      },
      "source": [
        "The above code use the dataframe `loc` function to select rows based on values in certain columns. Please refer to [this excellent article](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c) on how to use `loc` or `iloc`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY8qnAji_J_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_df.loc[dist_df['ID location 1'] == loc1[4]]['ID location 2'].values == loc1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMijILnMA4Cw",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to turn the `dist_df` into a dictionary `D` of vector. A key of `D` is some location id, the value corresponding to that key is a vector (1D numpy array) of distances to all 117 locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rIiRoV_DLQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spread = 2\n",
        "idc = inter_dist_class\n",
        "idc_dict = {idc[0]:(-2 + spread), idc[1]:(1 + spread), idc[2]:(2 + spread), \n",
        "            idc[3]:(0 + spread), idc[4]:(-1 + spread)}\n",
        "idc_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JTnprJ9BZ_n",
        "colab_type": "text"
      },
      "source": [
        "Now we have two options to use the location information. One is to directly create the distance matrix that we talked about during our extra tutorial. The other is to create a position vector for each measurement record in the main table. In this notebook, we go for the first option, in following notebooks, we will try both and compare them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjak160bBw7G",
        "colab_type": "text"
      },
      "source": [
        "##create distance matrix\n",
        "\n",
        "In this option, we \"recover\" the spatial coordinates of the windturbine based on their pair-wise distances. This might be possible with the [MDS method](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) to learn the vector embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULugYW_hB0Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_mat = np.zeros((len(loc1), len(loc1)))\n",
        "dist_mat.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T78joJzaCdlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, loc in enumerate(loc1):\n",
        "  dist_row = dist_df.loc[dist_df['ID location 1'] == loc]['Interdistance class'].values\n",
        "  dist_row = np.array([idc_dict[x] for x in dist_row], dtype=np.float)\n",
        "  dist_mat[i, :] = dist_row\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKzQk1oNDafc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_mat[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YndfvM6LE0Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import manifold\n",
        "from sklearn.metrics import euclidean_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSvkQE_7FMEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = np.random.RandomState(seed=3)\n",
        "mds = manifold.MDS(n_components=2, max_iter=4000, eps=1e-20, random_state=seed,\n",
        "                   dissimilarity=\"precomputed\", n_jobs=1, verbose=1, n_init=4)\n",
        "fit_re = mds.fit(dist_mat)\n",
        "pos, stress = fit_re.embedding_, fit_re.stress_\n",
        "stress\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyvVlaQtF02m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acbMLkLGDRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB-feG0NgWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist2 = euclidean_distances(pos)\n",
        "#print(dist2[0])\n",
        "delta_dist = np.abs(dist_mat - dist2)\n",
        "print(np.mean(delta_dist), np.mean(delta_dist) / np.mean(dist_mat))\n",
        "print(np.min(delta_dist), np.max(delta_dist))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXUc8mCeG1Ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_coord, y_coord = pos[:, 0], pos[:, 1]\n",
        "print(np.mean(x_coord), np.std(x_coord))\n",
        "print(np.mean(y_coord), np.std(y_coord))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVsGl_pyFZ4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.figure(figsize=(8, 8))\n",
        "plt.suptitle('\"Estimated\" layout of the wind turbine sites')\n",
        "a = plt.scatter(x_coord, y_coord)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ42JlWIGNFY",
        "colab_type": "text"
      },
      "source": [
        "The \"estimated\" layout shown above appears consistent with the \"V-shape\" [birds formation](https://www.sciencemag.org/news/2014/01/why-birds-fly-v-formation) in order to reach the aerodynamic sweet spot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeAY4cqtKWgl",
        "colab_type": "text"
      },
      "source": [
        "In order to use this location information, we create another dictionary for fast searching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyHH-pzIK_hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coord_dict = dict()\n",
        "for locname, loc_coord in zip(loc1, pos):\n",
        "  coord_dict[locname] = loc_coord"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTQNwPLmLpoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(coord_dict['BJ'], coord_dict['DE'])\n",
        "print(euclidean_distances(np.vstack([coord_dict['BJ'], coord_dict['DE']])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuroMsX4HDM6",
        "colab_type": "text"
      },
      "source": [
        "Now we need to add them back into the data frame for both training df and testing df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA0TuzTbHJOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_init_list = [0.0] * len(train_df)\n",
        "x_init_list[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ne2EWeuHWAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['x_coord'] = x_init_list\n",
        "train_df['y_coord'] = x_init_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsfJ5_LvHn-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNmKDZbOH1c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k, v in coord_dict.items():\n",
        "  train_df.loc[train_df['Location ID'] == k, 'x_coord'] = v[0]\n",
        "  train_df.loc[train_df['Location ID'] == k, 'y_coord'] = v[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_-N980-Is1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEQRfekRJxGB",
        "colab_type": "text"
      },
      "source": [
        "Do the same for validation / test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS2QGctRJz8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['x_coord'] = [0.0] * len(test_df)\n",
        "test_df['y_coord'] = [0.0] * len(test_df)\n",
        "for k, v in coord_dict.items():\n",
        "  test_df.loc[test_df['Location ID'] == k, 'x_coord'] = v[0]\n",
        "  test_df.loc[test_df['Location ID'] == k, 'y_coord'] = v[1]\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8oxQauUB4Gr",
        "colab_type": "text"
      },
      "source": [
        "##create position vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UgS1kHKwU8vc",
        "colab": {}
      },
      "source": [
        "spread = 2\n",
        "idc = inter_dist_class\n",
        "print(idc)\n",
        "idc_dict = {idc[0]:(-2 + spread), idc[1]:(1 + spread), idc[2]:(2 + spread), \n",
        "            idc[3]:(0 + spread), idc[4]:(-1 + spread)}\n",
        "idc_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg_5bxa4Edj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_dict = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL7JidLJCWzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for loc in loc1:\n",
        "  dist_row = dist_df.loc[dist_df['ID location 1'] == loc]['Interdistance class'].values\n",
        "  dist_row = np.array([idc_dict[x] for x in dist_row], dtype=np.float)\n",
        "  dist_dict[loc] = dist_row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjUGyLDwEqwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_dict[loc1[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd7BG6WAk4Y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_np = dist_df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH0R4B_jlAFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_np[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl7T2r4lc7sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(dist_df), 117 ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5pOC34x9jId",
        "colab_type": "text"
      },
      "source": [
        "We will **need to** use the distance information in the next iteration. But for now, we just leave it although we have a few good ideas how to use it. This basically means that we won't be able to use the location information in our initial solution. This is far from optimal, but it is fine to have a basic simple model to get us started quickly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvxA4OO8_j1W",
        "colab_type": "text"
      },
      "source": [
        "##Normalise data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YGsRceP380M",
        "colab_type": "text"
      },
      "source": [
        "We will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) in sklearn to normalise the data. The reason we need to normalise the data is clear, as you can see, the value range for each column is very different. We would like to standardise them so that the weights can be tuned in a uniform way without being biased towards absoluate magnitutde inherent in those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeJP0-0UAhCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcqrGRwdS3KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_df.columns)\n",
        "print()\n",
        "print(test_df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9UkZ4ZRJm5a",
        "colab_type": "text"
      },
      "source": [
        "Just to make sure we know the sequence of all the column names. We can't afford to make mistakes here (i.e. wrong order, or something)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKixe_864nCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_np = train_df.values\n",
        "X = train_np[:, [0, 1, 2, 3, 7, 8, 10, 11, 12, 13, 14]]\n",
        "print(X.shape, np.mean(X[:, 2]), np.std(X[:, 2]))\n",
        "y = train_np[:, [6]]\n",
        "print(y.shape, np.mean(y), np.std(y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRP4uwJCzD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_np = test_df.values\n",
        "X_test = test_np[:, [0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Phq66juIU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S63CEa11J3sB",
        "colab_type": "text"
      },
      "source": [
        "Please check the documentatio on the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Normalisation is essential for any ML projects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEXXZ3IXwjN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
        "scaler_x.fit(X)\n",
        "scaler_y.fit(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADkpxWorx02G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = scaler_x.transform(X)\n",
        "\n",
        "y = scaler_y.transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3wt1k7VMZal",
        "colab_type": "text"
      },
      "source": [
        "For the test dataset, we use a different scaler? Or should we? Need to verify that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUU3oyp8DibU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler_x_test = StandardScaler()\n",
        "scaler_x_test.fit(X_test)\n",
        "X_test = scaler_x.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvONgXrP1slZ",
        "colab_type": "text"
      },
      "source": [
        "Now check again if the data is zero-centred with a unit variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Q5tu6W1HK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape, np.mean(X[:, -1]), np.std(X[:, -1]))\n",
        "print(y.shape, np.mean(y), np.std(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kNa95CCsomQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAu9IacbttJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv1c3Xdr21LN",
        "colab_type": "text"
      },
      "source": [
        "Check if the validation set still follows the similar distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omVxFdhm2g22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.mean(X_val[:, -1]), np.std(X_val[:, -1]))\n",
        "print(np.mean(y_val), np.std(y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8diQsiahUZd",
        "colab_type": "text"
      },
      "source": [
        "#Fully-connected ANN model\n",
        "First, we use the plain linear regression model to understand the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlF9Z0umgSyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense,\\\n",
        "                         BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVGgSQPTMgCw",
        "colab_type": "text"
      },
      "source": [
        "We will use the [notion of CallBacks](https://medium.com/singlestone/keras-callbacks-monitor-and-improve-your-deep-learning-205a8a27e91c) to monitor our training progress, and save the \"best\" model against the validation set. However, do you think the size of the validation set is appropriate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSEE57pz512d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback_checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                                        monitor='val_loss',\n",
        "                                        verbose=1,\n",
        "                                        save_weights_only=True,\n",
        "                                        save_best_only=True)\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "model = Sequential(name='FC ANN')\n",
        "#model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu', \n",
        "#                name='first_hidden', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu', \n",
        "                name='first_hidden'))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(Dense(1024, activation='relu', name='second_hidden', kernel_regularizer=regularizers.l2(0.01)))\n",
        "#model.add(Dropout(0.25))\n",
        "model.add(Dense(64, activation='relu', name='third_hidden'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(1, name='blowcount', activation='linear'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=[root_mean_squared_error])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38zSr716IEyM",
        "colab_type": "text"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvwcpZJa7vNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                          validation_data=(X_val, y_val),\n",
        "                          epochs=300, batch_size=32, \n",
        "                          callbacks=[callback_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyI1ZaXpCKnH",
        "colab_type": "text"
      },
      "source": [
        "# Now test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlIxsaaLCQl3",
        "colab_type": "text"
      },
      "source": [
        "First, load the \"best\" model in terms of the validation error during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqTIWIRbYMy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp best_model.h5 best_model_11290.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-GY_3I589C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('best_model_11290.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKLhxttMCWwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHiiukRqDy63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "welVQC82D1IJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_submit = scaler_y.inverse_transform(test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5h2IoFtEDOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9nnoTcZEGV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_submit[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH6302M-EJK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit['Blowcount [Blows/m]'] = test_pred_submit[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsi67eRGQXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-hb8ZQmMGPo",
        "colab_type": "text"
      },
      "source": [
        "Please quickly glance your result before submitting it (don't waste your precious submission quota)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_euAUE9GSL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit.to_csv('submit_baseline_ann_04.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4tUz9ZkN_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions submit -c isfog2020-pile-driving-predictions -f submit_baseline_ann_04.csv -m \"add x y coord\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WrGZj_KNG-l",
        "colab_type": "text"
      },
      "source": [
        "#What's next\n",
        "\n",
        "How to treat PCT test as series of inter-dependent data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_itiBXRM_s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}