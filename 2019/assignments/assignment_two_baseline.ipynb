{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_two_baseline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ICRAR/PHYS5511/blob/master/2019/assignments/assignment_two_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMpLdOpKd9qH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "A \"naive\" baseline solution. The basic idea is very similar to the fully-connected neural network tutorial that we have developed back in [week 04](https://github.com/ICRAR/PHYS5511/blob/master/2019/week04/Keras_FC_network_classifier.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llajr0Rhx7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7L0Svfg7UZ",
        "colab_type": "text"
      },
      "source": [
        "#Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rahiH7cIJP0z",
        "colab_type": "text"
      },
      "source": [
        "The same usual \"business\" as Assignment One"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yFaWUahduJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRuIx4lNgzu1",
        "colab_type": "text"
      },
      "source": [
        "##Get the data for the first time\n",
        "You only need to run this once, and this step can be skipped for subsequent runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpQommDeeuZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My\\ Drive/PHYS5512/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35fjqmFfCoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir isfog2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLu5HikNfHJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd isfog2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyXSkdrlf1OL",
        "colab_type": "text"
      },
      "source": [
        "Make sure you already have the *kaggle.json* file, which you probably downloaded from Kaggle for assignment one. If not, you can download it from your Kaggle profile again. Then we copy that to the /root/.kaggle directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeDOJTnEfkgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ../../kaggle.json /root/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmIK-zB9fJ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c isfog2020-pile-driving-predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37FGA4mGfbMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH-H-0-tik8v",
        "colab_type": "text"
      },
      "source": [
        "##Goto the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fT-F3CVijXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My Drive/PHYS5512/data/isfog2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCQjm1SKhx7V",
        "colab_type": "text"
      },
      "source": [
        "#Pre-process data\n",
        "\n",
        "This part of the code of checking data is copied from the original kernel.\n",
        "\n",
        "The dataset is kindly provided by [Cathie Group](http://www.cathiegroup.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upigVCmq9QnZ",
        "colab_type": "text"
      },
      "source": [
        "##Importing data\n",
        "\n",
        "The first step in any data science exercise is to get familiar with the data. The data is provided in a csv file (```training_data.csv```). We can import the data with Pandas and display the first five rows using the ```head()``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctL_coJEhx7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"training_data_cleaned.csv\")  # Store the contents of the csv file in the variable 'train_df'\n",
        "train_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6IAjIU3hx7Y",
        "colab_type": "text"
      },
      "source": [
        "The data has 12 columns, containing PCPT data ($ q_c $, $ f_s $ and $ u_2 $), recorded hammer data (blowcount, normalised hammer energy, normalised ENTHRU and total number of blows), pile data (diameter, bottom wall thickness and pile final penetration). A unique ID identifies the location and $ z $ defines the depth below the mudline.\n",
        "\n",
        "The data has already been resampled to a regular grid with 0.5m grid intervals to facilitate the further data handling. Note that the \"grid\" axis is the depth.\n",
        "\n",
        "The hammer energy has been normalised using the same reference energy for all piles in this prediction exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ForaqA-4JF-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.groupby('Location ID')['Location ID'].agg(['count']).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jlH47qHob2f",
        "colab_type": "text"
      },
      "source": [
        "The above code checks how many records per location, here we only show the first 10 locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2WYB_6NrkUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_locs = train_df['Location ID'].unique()\n",
        "print(train_locs, len(train_locs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxeEZjOov4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit = pd.read_csv(\"sample_submission.csv\")\n",
        "df_submit.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD9wkwGH5S9g",
        "colab_type": "text"
      },
      "source": [
        "The submission contains two columns. The first column is a contatination the location ID of the wind turbine and the pile depth. The second column is the predicted blowcount per meter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhG-gVtJFlUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(df_submit), df_submit.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7nXISc9570V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(\"validation_data_cleaned.csv\")\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHyU5Kb8eT7",
        "colab_type": "text"
      },
      "source": [
        "The test dataset 10 columns - the PCPT data ($ q_c $, $ f_s $ and $ u_2 $), **incomplete** hammer data (normalised hammer energy, normalised ENTHRU), pile data (diameter, bottom wall thickness and pile final penetration). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SEYeKsVpAkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.groupby('Location ID')['Location ID'].agg(['count']).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzyU6qwBsKMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_locs = test_df['Location ID'].unique()\n",
        "print(test_locs, len(test_locs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHbgPdxmI_Ec",
        "colab_type": "text"
      },
      "source": [
        "The above code checks unique locations included in the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyaaz3w9pNiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_df = pd.read_csv(\"interdistance_data.csv\")\n",
        "dist_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5pOC34x9jId",
        "colab_type": "text"
      },
      "source": [
        "We will **need to** use the distance information in the next iteration. But for now, we just leave it although we have a few good ideas how to use it. This basically means that we won't be able to use the location information in our initial solution. This is far from optimal, but it is fine to have a basic simple model to get us started quickly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvxA4OO8_j1W",
        "colab_type": "text"
      },
      "source": [
        "##Normalise data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YGsRceP380M",
        "colab_type": "text"
      },
      "source": [
        "We will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) in sklearn to normalise the data. The reason we need to normalise the data is clear, as you can see, the value range for each column is very different. We would like to standardise them so that the weights can be tuned in a uniform way without being biased towards absoluate magnitutde inherent in those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeJP0-0UAhCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcqrGRwdS3KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_df.columns)\n",
        "print()\n",
        "print(test_df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9UkZ4ZRJm5a",
        "colab_type": "text"
      },
      "source": [
        "Just to make sure we know the sequence of all the column names. We can't afford to make mistakes here (i.e. wrong order, or something)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKixe_864nCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_np = train_df.values\n",
        "X = train_np[:, [0, 1, 2, 3, 7, 8, 10, 11, 12]]\n",
        "print(X.shape, np.mean(X[:, 2]), np.std(X[:, 2]))\n",
        "y = train_np[:, [6]]\n",
        "print(y.shape, np.mean(y), np.std(y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRP4uwJCzD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_np = test_df.values\n",
        "X_test = test_np[:, [0, 1, 2, 3, 6, 7, 8, 9, 10]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Phq66juIU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S63CEa11J3sB",
        "colab_type": "text"
      },
      "source": [
        "Please check the documentatio on the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Normalisation is essential for any ML projects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEXXZ3IXwjN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
        "scaler_x.fit(X)\n",
        "scaler_y.fit(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADkpxWorx02G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = scaler_x.transform(X)\n",
        "\n",
        "y = scaler_y.transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3wt1k7VMZal",
        "colab_type": "text"
      },
      "source": [
        "For the test dataset, we use a different scaler? Or should we? Need to verify that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUU3oyp8DibU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler_x_test = StandardScaler()\n",
        "scaler_x_test.fit(X_test)\n",
        "X_test = scaler_x.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvONgXrP1slZ",
        "colab_type": "text"
      },
      "source": [
        "Now check again if the data is zero-centred with a unit variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Q5tu6W1HK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape, np.mean(X[:, 2]), np.std(X[:, 2]))\n",
        "print(y.shape, np.mean(y), np.std(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kNa95CCsomQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAu9IacbttJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape, X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv1c3Xdr21LN",
        "colab_type": "text"
      },
      "source": [
        "Check if the validation set still follows the similar distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omVxFdhm2g22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.mean(X_val[:, 2]), np.std(X_val[:, 2]))\n",
        "print(np.mean(y_val), np.std(y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8diQsiahUZd",
        "colab_type": "text"
      },
      "source": [
        "#Fully-connected ANN model\n",
        "First, we use the plain linear regression model to understand the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlF9Z0umgSyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense,\\\n",
        "                         BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVGgSQPTMgCw",
        "colab_type": "text"
      },
      "source": [
        "We will use the [notion of CallBacks](https://medium.com/singlestone/keras-callbacks-monitor-and-improve-your-deep-learning-205a8a27e91c) to monitor our training progress, and save the \"best\" model against the validation set. However, do you think the size of the validation set is appropriate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47VuT3B2BhiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback_checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                                        monitor='val_loss',\n",
        "                                        verbose=1,\n",
        "                                        save_weights_only=True,\n",
        "                                        save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSEE57pz512d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def root_mean_squared_error(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "model = Sequential(name='FC ANN')\n",
        "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu', \n",
        "                name='first_hidden'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu', name='second_hidden'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(32, activation='relu', name='third_hidden'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(1, name='blowcount', activation='linear'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=[root_mean_squared_error])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38zSr716IEyM",
        "colab_type": "text"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvwcpZJa7vNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                          validation_data=(X_val, y_val),\n",
        "                          epochs=100, batch_size=32, \n",
        "                          callbacks=[callback_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyI1ZaXpCKnH",
        "colab_type": "text"
      },
      "source": [
        "# Now test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlIxsaaLCQl3",
        "colab_type": "text"
      },
      "source": [
        "First, load the \"best\" model in terms of the validation error during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-GY_3I589C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('best_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKLhxttMCWwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHiiukRqDy63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "welVQC82D1IJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_submit = scaler_y.inverse_transform(test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5h2IoFtEDOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9nnoTcZEGV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_submit[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH6302M-EJK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit['Blowcount [Blows/m]'] = test_pred_submit[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsi67eRGQXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-hb8ZQmMGPo",
        "colab_type": "text"
      },
      "source": [
        "Please quickly glance your result before submitting it (don't waste your precious submission quota)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_euAUE9GSL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submit.to_csv('submit_baseline_ann_01.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WrGZj_KNG-l",
        "colab_type": "text"
      },
      "source": [
        "#What's next\n",
        "\n",
        "1. How to add location information\n",
        "   A promising direction is to \"recover\" the spatial coordinates of the windturbine based on their pair-wise distances. This might be possible with the [MDS method](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) to learn the vector embeddings.\n",
        "2. How to treat PCT test as series of inter-dependent data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC36z6yXNITb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}